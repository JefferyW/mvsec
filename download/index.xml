<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Downloads on Multi Vehicle Stereo Event Camera Dataset</title>
    <link>https://daniilidis-group.github.io/mvsec/download/</link>
    <description>Recent content in Downloads on Multi Vehicle Stereo Event Camera Dataset</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Aug 2017 21:07:13 +0100</lastBuildDate>
    <atom:link href="https://daniilidis-group.github.io/mvsec/download/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Download</title>
      <link>https://daniilidis-group.github.io/mvsec/download/</link>
      <pubDate>Tue, 08 Aug 2017 21:07:13 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/mvsec/download/</guid>
      <description>

&lt;h2 id=&#34;hdf5-files&#34;&gt;HDF5 Files&lt;/h2&gt;

&lt;p&gt;The data streams from the individual sensors have been combined into hdf5 files that mirror the ROS bag structure. hdf5 is a standard format with support in almost any language, and should enable easier development for non-ROS users.&lt;/p&gt;

&lt;p&gt;The timestamps for each topic are embedded as an additional array with suffix &amp;lsquo;_ts&amp;rsquo;. For example in python, loading a file and reading the left grayscale images and timestamps would involve the following lines of code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import h5py
data = h5py.File(&#39;outdoor_day2_data.hdf5&#39;)
images = data[&#39;davis&#39;][&#39;left&#39;][&#39;image_raw&#39;]
image_ts = data[&#39;davis&#39;][&#39;left&#39;][&#39;image_raw_ts&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition, we provide a mapping for the nearest event to each DAVIS image in time, as, for example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image_raw_event_inds = data[&#39;davis&#39;][&#39;left&#39;][&#39;image_raw_event_inds&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;, where &lt;code&gt;image_raw_event_inds[image_ind]&lt;/code&gt; would be the event index corresponding to image &lt;code&gt;image_ind&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Note that the events are concatenated into a single array, and as such do not have the associated ROS message timestamps. However, each individual event retains its timestamp.&lt;/p&gt;

&lt;p&gt;The files can be found in the Google Drive folder here: &lt;a href=&#34;https://drive.google.com/open?id=1rwyRk26wtWeRgrAx_fgPc-ubUzTFThkV&#34;&gt;https://drive.google.com/open?id=1rwyRk26wtWeRgrAx_fgPc-ubUzTFThkV&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;ros-bags&#34;&gt;ROS Bags&lt;/h2&gt;

&lt;p&gt;To process the bag files, you will need the &lt;a href=&#34;https://github.com/uzh-rpg/rpg_dvs_ros&#34;&gt;rpg_dvs_ros&lt;/a&gt; package to read the events (in particular dvs_msgs). You may also optionally install the &lt;a href=&#34;https://github.com/ethz-asl/visensor_node&#34;&gt;visensor_node&lt;/a&gt; to have access to the /cust_imu0 topic, which includes the magnetometer, pressure and temperature outputs of the VI-Sensor.&lt;/p&gt;

&lt;p&gt;Sequences will be added to this page on a rolling basis. We also plan to include videos for each sequence.&lt;/p&gt;

&lt;p&gt;Note that these bags are large (up to 27G).&lt;/p&gt;

&lt;p&gt;If the server is down (links no longer work), the individual files can be found &lt;a href=&#34;https://drive.google.com/drive/folders/1gDy2PwVOu_FPOsEZjojdWEB2ZHmpio8D?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&#39;float:left;margin-left:5%&#39;&gt;
&lt;table&gt;
&lt;col width=&#34;30%&#34;&gt;
&lt;col width=&#34;10%&#34;&gt;
&lt;col width=&#34;50%&#34;&gt;
&lt;tr&gt;&lt;td&gt;Scene&lt;/td&gt;&lt;td&gt;Calibration&lt;/td&gt;&lt;td&gt;Sequence&lt;/td&gt;&lt;td&gt;Map/Image&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Indoor flying (Note: No VI-Sensor data is available for this scene).&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flying_calib.zip&#34;&gt;Calibration&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
Indoor Flying 1 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flying1_data.bag&#34;&gt;Data (1.2G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flying1_gt.bag&#34;&gt;Ground truth (2.6G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
Indoor Flying 2 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flying2_data.bag&#34;&gt;Data (1.7G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flying2_gt.bag&#34;&gt;Ground truth (3.2G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
Indoor Flying 3 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flying3_data.bag&#34;&gt;Data (1.8G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flying3_gt.bag&#34;&gt;Ground truth (3.5G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
Indoor Flying 4 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flying4_data.bag&#34;&gt;Data (419M)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flying4_gt.bag&#34;&gt;Ground truth (738M)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Outdoor Driving Day (Note: A hardware failure caused the grayscale images on the right DAVIS grayscale images for this scene to be corrupted. However, VI-Sensor grayscale images are available).&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/outdoor_day_calib.zip&#34;&gt;Calibration&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
Outdoor Day 1 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/outdoor_day1_data.bag&#34;&gt;Data (9.7G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/outdoor_day1_gt.bag&#34;&gt;Ground truth (9.5G)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a target=&#34;_blank&#34; href=&#34;../figs/gt_maps/west_philly_day1_traj.jpg&#34;&gt;
&lt;img src=&#34;../figs/gt_maps/west_philly_day1_traj.jpg&#34; alt=&#34;outdoor_day1&#34; style=&#34;max-height:150px&#34;/&gt;
&lt;/a&gt;
&lt;/td&gt;

&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
Outdoor Day 2 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/outdoor_day2_data.bag&#34;&gt;Data (27G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/outdoor_day2_gt.bag&#34;&gt;Ground truth (23G)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a target=&#34;_blank&#34; href=&#34;../figs/gt_maps/west_philly_day2_traj.jpg&#34;&gt;
&lt;img src=&#34;../figs/gt_maps/west_philly_day2_traj.jpg&#34; alt=&#34;outdoor_day2&#34; style=&#34;max-height:150px&#34;/&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Outdoor Driving Night&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/outdoor_night_calib.zip&#34;&gt;Calibration&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
Outdoor Night 1 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/outdoor_night1_data.bag&#34;&gt;Data (8.1G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/outdoor_night1_gt.bag&#34;&gt;Ground truth (9.5G)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a target=&#34;_blank&#34; href=&#34;../figs/gt_maps/west_philly_night1_traj.jpg&#34;&gt;
&lt;img src=&#34;../figs/gt_maps/west_philly_night1_traj.jpg&#34; alt=&#34;outdoor_night1&#34; style=&#34;max-height:150px&#34;/&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
Outdoor Night 2 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/outdoor_night2_data.bag&#34;&gt;Data (11G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/outdoor_night2_gt.bag&#34;&gt;Ground truth (11G)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a target=&#34;_blank&#34; href=&#34;../figs/gt_maps/west_philly_night2_traj.jpg&#34;&gt;
&lt;img src=&#34;../figs/gt_maps/west_philly_night2_traj.jpg&#34; alt=&#34;outdoor_night2&#34; style=&#34;max-height:150px&#34;/&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
Outdoor Night 3 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/outdoor_night3_data.bag&#34;&gt;Data (9G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/outdoor_night3_gt.bag&#34;&gt;Ground truth (11G)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a target=&#34;_blank&#34; href=&#34;../figs/gt_maps/west_philly_night3_traj.jpg&#34;&gt;
&lt;img src=&#34;../figs/gt_maps/west_philly_night3_traj.jpg&#34; alt=&#34;outdoor_night3&#34; style=&#34;max-height:150px&#34;/&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Motorcycle (Note: No lidar).&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/motorcycle/motorcycle_calib.zip&#34;&gt;Calibration&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
Highway 1 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/motorcycle/motorcycle_data.bag&#34;&gt;Data (42G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/motorcycle/motorcycle_gt.bag&#34;&gt;Ground truth (659K)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;BR CLEAR=&#34;all&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;ground-truth-optical-flow-generation&#34;&gt;Ground Truth Optical Flow Generation&lt;/h2&gt;

&lt;p&gt;In addition to the ground truth provided by the original dataset, we provide code to generate dense ground truth optical flow for each sequence with ground truth poses and depths. For storage and bandwidth reasons, we do not provide the optical flow directly, but instead provide the code to generate it from the ground truth provided here. The method for this is outlined in the paper:
&lt;br&gt;&lt;a href=&#34;http://www.roboticsproceedings.org/rss14/p62.pdf&#34;&gt;EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The processed optical flow ground truth has been saved in numpy format (.npz), and can be found &lt;a href=&#34;https://drive.google.com/drive/u/2/folders/1XS0AQTuCwUaWOmtjyJWRHkbXjj_igJLp&#34;&gt;here&lt;/a&gt;. The ground truth flow for each sequence has a suffix of &lt;code&gt;_gt_flow_dist&lt;/code&gt;. Each npz file contains a dictionary with keys: &lt;code&gt;&#39;timestamps&#39;&lt;/code&gt;, &lt;code&gt;&#39;x_flow_dist&#39;&lt;/code&gt;, &lt;code&gt;&#39;y_flow_dist&#39;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We also provide files with suffix &lt;code&gt;_odom&lt;/code&gt;, which contains a dictionary with keys: &lt;code&gt;&#39;timestamps&#39;&lt;/code&gt;, &lt;code&gt;&#39;lin_vel&#39;&lt;/code&gt;, &lt;code&gt;&#39;ang_vel&#39;&lt;/code&gt;, &lt;code&gt;&#39;pos&#39;&lt;/code&gt;, &lt;code&gt;&#39;quat&#39;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The git repo for this ground truth can be found here:
&lt;a href=&#34;https://github.com/daniilidis-group/mvsec/tree/master/tools/gt_flow&#34;&gt;https://github.com/daniilidis-group/mvsec/tree/master/tools/gt_flow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you use this optical flow dataset, please cite:&lt;/p&gt;

&lt;p&gt;Zhu, A. Z., Yuan, L., Chaney, K., Daniilidis, K. (2018). &lt;a href=&#34;http://www.roboticsproceedings.org/rss14/p62.pdf&#34;&gt;EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras&lt;/a&gt; Robotics: Science and Systems 2018.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>