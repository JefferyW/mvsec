---
date: 2017-08-08T21:07:13+01:00
title: The MVSEC Data Set
type: index
weight: 0
---
## The Multi Vehicle Stereo Event Camera Dataset

<img src="figs/sensor_mounts.jpg" alt="Sensors mounted on multiple vehicles." style="height: 250px;"/>

The Multi Vehicle Stereo Event Camera dataset is a collection of data designed for the development of novel 3D perception algorithms for **event based cameras**. Stereo event data is collected from car, motorbike, hexacopter and handheld data, and fused with lidar, IMU, motion capture and GPS to provide ground truth pose and depth images. In addition, we provide images from a standard stereo frame based camera pair for comparison with traditional techniques.

Event based cameras are a new **asynchronous** sensing modality that measure changes in image intensity. When the log intensity over a pixel changes above a set threshold, the camera **immediately** returns the pixel location of a change, along with a timestamp with **microsecond** accuracy, and the direction of the change (up or down). This allows for sensing with extremely low latency. In addition, the cameras have extremely **high dynamic range** and **low power usage**.

## Sequences

<table class="image">
<tr><td>
<img src="figs/vicon_image.png" alt="Hexacopter Indoor." style="width: 100%;"/>
</td>

<td>
<img src="figs/qualisys_imag.png" alt="Hexacopter Outdoor." style="width: 100%;"/>
</td>

<td>
<img src="figs/indoor_outdoor.png" alt="Handheld." style="width: 100%;"/>
</td>
</tr>
<tr>
<td class="caption">
<center>Hexacopter Indoor</center>
</td>
<td class="caption">
<center>Hexacopter Outdoor</center>
</td>
<td class="caption">
<center>Handheld</center>
</td>
<tr>
<td>
<img src="figs/driving_day_image.png" alt="Daytime Driving." style="width: 100%;"/>
</td>
<td>
<img src="figs/driving_night_image.png" alt="Nighttime Driving.." style="width: 100%;"/>
</td>
</tr>
<tr>
<td class="caption">
<center>Daytime Driving</center>
</td>
<td class="caption">
<center>Nighttime Driving</center>
</td>
</tr>
</table>
</center>

Data was collected from four different vehicles, in both indoor and outdoor environments, in day and night settings. All hexacopter sequences have motion capture ground truth from an indoor Vicon area and outdoor Qualisys area, while the other sequences have ground truth generated by fusing lidar information with IMU and GPS.
The full list of sequences can be found below: 

<div style='float:left;margin-left:5%'>
<table>
<tr><td>Vehicle</td><td>Sequence</td></tr>
<tr>
<td>Hexacopter</td>
<td>
<ul>
<li>Indoor short</li>
<li>Indoor long</li>
<li>Outdoor afternoon</li>
<li>Outdoor evening</li>
</ul>
</td>
</tr>
<tr>
<td>Handheld</td>
<td>
<ul>
<li>Indoor-outdoor</li>
<li>Outdoor-indoor</li>
</ul>
</td>
</tr>
<tr>
<td>Outdoor Car</td>
<td>
<ul>
<li>Pennovations day</li>
<li>Pennovations evening</li>
<li>West Philadelphia day</li>
<li>West Philadelphia evening</li>
</ul>
</td>
</tr>
<tr>
<td>Motorcycle</td>
<td>
<ul>
<li>Motorcycle highway</li>
</ul>
</td>
</tr>
</table>
</div>

<div style='margin-left:50%;margin-top:5%;' >
<img src="figs/vicon.jpg" alt="Indoor Vicon motion capture area." style="width: 400px;"/>
<center>Indoor Vicon motion capture area.</center>
<img src="figs/qualisys.jpg" alt="Outdoor Qualisys motion capture area." style="width: 400px;"/>
<center>Outdoor Qualisys motion capture area.</center>
</div>

<BR CLEAR="all">

<h2>Sensors</h2>

<img src="figs/car_sensor_rig.png" alt="Full sensor configuration." style="height: 300px;" align="right"/>

A number of different sensors and modalities and rigidly mounted to a stereo event camera pair, in order to generate accurate ground truth information, as well as to provide avenues for research in sensor fusion between modalities.
<br/>
<br/>
For events, two experimental DAVIS 346B cameras are mounted in a stereo (X axes aligned) configuration. each camera has a resolution of 346x260 pixels, with a 4mm lens and roughly 70 degrees vertical field of view. The camera clocks are synchronized by a hardware trigger generated by the left camera and send to the right camera. In addition to events, the cameras also each generate IMU and frame based image measurements.
<br/>
<br/>
In addition, a Velodyne lidar and stereo frame based camera with IMU (VI Sensor) is mounted with the DAVIS cameras. When available, ground truth pose is also captured using an indoor (Vicon, left) or outdoor (Qualisys, right) motion capture system.
<br/>
<br/>
The full set of sensor characteristics can be found below:

<table>
<tr><td>Sensor</td><td>Characteristics</td></tr>
<tr>
<td>DAVIS 346B</td>
<td>
<ul>
<li>346x260 pixels</li>
<li>APS (Active Pixel Sensor for frame based images)</li>
<li>DVS (Dynamic Vision Sensor for events)</li>
<li>FOV: 50&deg vert., 65&deg horiz.</li>
<li>IMU: MPU 6150 at 1kHz</li>
</ul>
</td>
</tr>
<tr>
<td>VI-Sensor</td>
<td>
<ul>
<li>Skybotix integrated VI-sensor</li>
<li>stereo camera: 2 x Aptina MT9V034</li>
<li>gray 2x752x480 at 20fps (rectified), global shutter</li>
<li>FOV: 57deg vert., 2 x 80deg horiz.</li>
<li>IMU: ADIS16488 at 200Hz</li>
</ul>
</td>
</tr>
<tr>
<td>Velodyne Puck LITE</td>
<td>
<ul>
<li>VLP-16 PUCK LITE</li>
<li>FOV: 30&deg vert. 360&deg horiz.</li>
<li>16 channel</li>
<li>20Hz</li>
<li>100m range</li>
</ul>
</td>
</tr>
<tr>
<td>GPS</td>
<td>
<ul>
<li>UBLOX NEO-M8N</li>
<li>72-channel u-blox M8 engine</li>
<li>Position accuracy 2.0m CEP</li>
</ul>
</td>
</tr>
<td>Motion Capture</td>
<td>
<ul>
<li>Indoor Vicon</li>
<li>88 x 22 x 15 ft</li>
<li>20 Vicon Vantage VP-16 Cameras</li>
<li>100Hz pose updates</li>
<li>Outdoor Qualisys</li>
<li>100 x 50 x 50 ft</li>
<li>34 Qualisys Opus 700 Cameras</li>
<li>100Hz pose updates</li>
</ul>
</td>
</tr>
</table>

## Ground Truth

For most sequences, accurate pose and depths are provided from a fusion of the sensors onboard.

![map with pose](figs/perch_map.png)
<center>Reconstructed map with trajectory in green.</center>
![depth image](figs/depth_blend.png)
<center>Depth map (red) overlaid on APS from DAVIS, lighter is further.</center>

## Citations

This dataset is currently undergoing the review process. Please email the authors at alexzhu (at) seas.upenn.edu if you plan to use this dataset in a published work for the relevant citation.