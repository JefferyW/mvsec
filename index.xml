<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multi Vehicle Stereo Event Camera Dataset</title>
    <link>https://daniilidis-group.github.io/mvsec/</link>
    <description>Recent content on Multi Vehicle Stereo Event Camera Dataset</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Aug 2017 21:07:13 +0100</lastBuildDate>
    <atom:link href="https://daniilidis-group.github.io/mvsec/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The MVSEC Data Set</title>
      <link>https://daniilidis-group.github.io/mvsec/</link>
      <pubDate>Tue, 08 Aug 2017 21:07:13 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/mvsec/</guid>
      <description>

&lt;h2 id=&#34;the-multi-view-stereo-event-camera-dataset&#34;&gt;The Multi View Stereo Event Camera Dataset&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;figs/sensor_mounts.jpg&#34; alt=&#34;Sensors mounted on multiple vehicles.&#34; style=&#34;height: 250px;&#34;/&gt;&lt;/p&gt;

&lt;p&gt;This website is currently being built while we are waiting to get the
data storage online. Please check back soon!&lt;/p&gt;

&lt;p&gt;The Multi Vehicle Stereo Event Camera dataset is a collection of data designed for the development of novel 3D perception algorithms for &lt;strong&gt;event based cameras&lt;/strong&gt;. Stereo event data is collected from car, motorbike, hexacopter and handheld data, and fused with lidar, IMU, motion capture and GPS to provide ground truth pose and depth images. In addition, we provide images from a standard stereo frame based camera pair for comparison with traditional techniques.&lt;/p&gt;

&lt;p&gt;Event based cameras are a new &lt;strong&gt;asynchronous&lt;/strong&gt; sensing modality that measure changes in image intensity. When the log intensity over a pixel changes above a set threshold, the camera &lt;strong&gt;immediately&lt;/strong&gt; returns the pixel location of a change, along with a timestamp with &lt;strong&gt;microsecond&lt;/strong&gt; accuracy, and the direction of the change (up or down). This allows for sensing with extremely low latency. In addition, the cameras have extremely &lt;strong&gt;high dynamic range&lt;/strong&gt; and &lt;strong&gt;low power usage&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;sequences&#34;&gt;Sequences&lt;/h2&gt;

&lt;table class=&#34;image&#34;&gt;
&lt;tr&gt;&lt;td&gt;
&lt;img src=&#34;figs/vicon_image.png&#34; alt=&#34;Hexacopter Indoor.&#34; style=&#34;width: 100%;&#34;/&gt;
&lt;/td&gt;

&lt;td&gt;
&lt;img src=&#34;figs/qualisys_imag.png&#34; alt=&#34;Hexacopter Outdoor.&#34; style=&#34;width: 100%;&#34;/&gt;
&lt;/td&gt;

&lt;td&gt;
&lt;img src=&#34;figs/indoor_outdoor.png&#34; alt=&#34;Handheld.&#34; style=&#34;width: 100%;&#34;/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;caption&#34;&gt;
&lt;center&gt;Hexacopter Indoor&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&#34;caption&#34;&gt;
&lt;center&gt;Hexacopter Outdoor&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&#34;caption&#34;&gt;
&lt;center&gt;Handheld&lt;/center&gt;
&lt;/td&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;img src=&#34;figs/driving_day_image.png&#34; alt=&#34;Daytime Driving.&#34; style=&#34;width: 100%;&#34;/&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;img src=&#34;figs/driving_night_image.png&#34; alt=&#34;Nighttime Driving..&#34; style=&#34;width: 100%;&#34;/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;caption&#34;&gt;
&lt;center&gt;Daytime Driving&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&#34;caption&#34;&gt;
&lt;center&gt;Nighttime Driving&lt;/center&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;

Data was collected from four different vehicles, in both indoor and outdoor environments, in day and night settings. All hexacopter sequences have motion capture ground truth from an indoor Vicon area and outdoor Qualisys area, while the other sequences have ground truth generated by fusing lidar information with IMU and GPS.
The full list of sequences can be found below: 

&lt;div style=&#39;float:left;margin-left:5%&#39;&gt;
&lt;table&gt;
&lt;tr&gt;&lt;td&gt;Vehicle&lt;/td&gt;&lt;td&gt;Sequence&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hexacopter&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Indoor short&lt;/li&gt;
&lt;li&gt;Indoor long&lt;/li&gt;
&lt;li&gt;Outdoor afternoon&lt;/li&gt;
&lt;li&gt;Outdoor evening&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Handheld&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Indoor-outdoor&lt;/li&gt;
&lt;li&gt;Outdoor-indoor&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Outdoor Car&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Pennovations day&lt;/li&gt;
&lt;li&gt;Pennovations evening&lt;/li&gt;
&lt;li&gt;West Philadelphia day&lt;/li&gt;
&lt;li&gt;West Philadelphia evening&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Motorcycle&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Motorcycle highway&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div style=&#39;margin-left:50%;margin-top:5%;&#39; &gt;
&lt;img src=&#34;figs/vicon.jpg&#34; alt=&#34;Indoor Vicon motion capture area.&#34; style=&#34;width: 400px;&#34;/&gt;
&lt;center&gt;Indoor Vicon motion capture area.&lt;/center&gt;
&lt;img src=&#34;figs/qualisys.jpg&#34; alt=&#34;Outdoor Qualisys motion capture area.&#34; style=&#34;width: 400px;&#34;/&gt;
&lt;center&gt;Outdoor Qualisys motion capture area.&lt;/center&gt;
&lt;/div&gt;

&lt;BR CLEAR=&#34;all&#34;&gt;

&lt;h2&gt;Sensors&lt;/h2&gt;

&lt;img src=&#34;figs/car_sensor_rig.png&#34; alt=&#34;Full sensor configuration.&#34; style=&#34;height: 300px;&#34; align=&#34;right&#34;/&gt;

A number of different sensors and modalities and rigidly mounted to a stereo event camera pair, in order to generate accurate ground truth information, as well as to provide avenues for research in sensor fusion between modalities.
&lt;br/&gt;
&lt;br/&gt;
For events, two experimental DAVIS 346B cameras are mounted in a stereo (X axes aligned) configuration. each camera has a resolution of 346x260 pixels, with a 4mm lens and roughly 70 degrees vertical field of view. The camera clocks are synchronized by a hardware trigger generated by the left camera and send to the right camera. In addition to events, the cameras also each generate IMU and frame based image measurements.
&lt;br/&gt;
&lt;br/&gt;
In addition, a Velodyne lidar and stereo frame based camera with IMU (VI Sensor) is mounted with the DAVIS cameras. When available, ground truth pose is also captured using an indoor (Vicon, left) or outdoor (Qualisys, right) motion capture system.
&lt;br/&gt;
&lt;br/&gt;
The full set of sensor characteristics can be found below:

&lt;table&gt;
&lt;tr&gt;&lt;td&gt;Sensor&lt;/td&gt;&lt;td&gt;Characteristics&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DAVIS 346B&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;346x260 pixels&lt;/li&gt;
&lt;li&gt;APS (Active Pixel Sensor for frame based images)&lt;/li&gt;
&lt;li&gt;DVS (Dynamic Vision Sensor for events)&lt;/li&gt;
&lt;li&gt;FOV: 50&amp;deg vert., 65&amp;deg horiz.&lt;/li&gt;
&lt;li&gt;IMU: MPU 6150 at 1kHz&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VI-Sensor&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Skybotix integrated VI-sensor&lt;/li&gt;
&lt;li&gt;stereo camera: 2 x Aptina MT9V034&lt;/li&gt;
&lt;li&gt;gray 2x752x480 at 20fps (rectified), global shutter&lt;/li&gt;
&lt;li&gt;FOV: 57deg vert., 2 x 80deg horiz.&lt;/li&gt;
&lt;li&gt;IMU: ADIS16488 at 200Hz&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Velodyne Puck LITE&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;VLP-16 PUCK LITE&lt;/li&gt;
&lt;li&gt;FOV: 30&amp;deg vert. 360&amp;deg horiz.&lt;/li&gt;
&lt;li&gt;16 channel&lt;/li&gt;
&lt;li&gt;20Hz&lt;/li&gt;
&lt;li&gt;100m range&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPS&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;UBLOX NEO-M8N&lt;/li&gt;
&lt;li&gt;72-channel u-blox M8 engine&lt;/li&gt;
&lt;li&gt;Position accuracy 2.0m CEP&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;td&gt;Motion Capture&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Indoor Vicon&lt;/li&gt;
&lt;li&gt;88 x 22 x 15 ft&lt;/li&gt;
&lt;li&gt;20 Vicon Vantage VP-16 Cameras&lt;/li&gt;
&lt;li&gt;100Hz pose updates&lt;/li&gt;
&lt;li&gt;Outdoor Qualisys&lt;/li&gt;
&lt;li&gt;100 x 50 x 50 ft&lt;/li&gt;
&lt;li&gt;34 Qualisys Opus 700 Cameras&lt;/li&gt;
&lt;li&gt;100Hz pose updates&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&#34;ground-truth&#34;&gt;Ground Truth&lt;/h2&gt;

&lt;p&gt;For most sequences, accurate pose and depths are provided from a fusion of the sensors onboard.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;figs/perch_map.png&#34; alt=&#34;map with pose&#34; /&gt;
&lt;center&gt;Reconstructed map with trajectory in green.&lt;/center&gt;
&lt;img src=&#34;figs/depth_blend.png&#34; alt=&#34;depth image&#34; /&gt;
&lt;center&gt;Depth map (red) overlaid on APS from DAVIS, lighter is further.&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;citations&#34;&gt;Citations&lt;/h2&gt;

&lt;p&gt;This dataset is currently undergoing the review process. Please email the authors at alexzhu (at) seas.upenn.edu if you plan to use this dataset in a published work for the relevant citation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calibration</title>
      <link>https://daniilidis-group.github.io/mvsec/calibration_format/</link>
      <pubDate>Tue, 08 Aug 2017 21:07:13 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/mvsec/calibration_format/</guid>
      <description>

&lt;h2 id=&#34;calibration-parameters&#34;&gt;Calibration Parameters&lt;/h2&gt;

&lt;p&gt;Each camera was intrinsically calibrated using &lt;a href=&#34;https://github.com/ethz-asl/kalibr&#34;&gt;Kalibr&lt;/a&gt;, with the DAVIS images calibrated using the equidistant distortion model, and the VI-Sensor images calibrated using the standard radtan distortion model. The two different distortion models is due to the slightly smaller focal length (more fisheye) lenses used on the DAVIS cameras compared to the stock VI-Sensor lenses.&lt;/p&gt;

&lt;p&gt;To rectify the VI-Sensor images, you can use the standard OpenCV or ROS rectification functions.&lt;/p&gt;

&lt;p&gt;To rectify the DAVIS images and events, you will need to use the &lt;a href=&#34;https://docs.opencv.org/3.0-beta/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#fisheye&#34;&gt;OpenCV fisheye rectification functions&lt;/a&gt;. This amounts to simply adding the fisheye namespace in front of the usual function (e.g. cv::fisheye::undistortPoints vs cv::undistortPoints). Note that the same cv::remap function works on both sets of images (no fisheye namespace needed). ROS does not currently support the equidistant distortion model. However, you can look at these pull requests: &lt;a href=&#34;https://github.com/ros/common_msgs/pull/109&#34;&gt;one&lt;/a&gt;, &lt;a href=&#34;https://github.com/ros-perception/vision_opencv/pull/184&#34;&gt;two&lt;/a&gt;, to the common_msgs and vision_opencv repos to find changes to the ROS image processing pipeline that allow for this model. Once these pull requests are merged in, this will no longer be necessary.&lt;/p&gt;

&lt;p&gt;The extrinsics between the lidar and the left DAVIS camera are provided, as well as extrinsics between all cameras, as well as between each camera and its own IMU. In addition, the ground truth pose has been transformed into the left DAVIS camera frame.&lt;/p&gt;

&lt;p&gt;All intrinsic and extrinsic calibrations are stored in &lt;strong&gt;yaml&lt;/strong&gt; format, roughly following the calibration yaml files output from &lt;a href=&#34;https://github.com/ethz-asl/kalibr&#34;&gt;Kalibr&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;calibration-file-format&#34;&gt;Calibration File Format&lt;/h2&gt;

&lt;p&gt;Each scene (corresponding to a single day of recording) has its own calibration file. Each file consists of:
&lt;ul&gt;
  &lt;li&gt;T_cam0_lidar: The 4x4 transformation that takes a point from the Velodyne frame to the left DAVIS camera frame.&lt;/li&gt;
  &lt;li&gt;For each camera (0-3):
  &lt;ul&gt;
    &lt;li&gt;Distortion model and coefficients&lt;/li&gt;
    &lt;li&gt;Intrinsics&lt;/li&gt;
    &lt;li&gt;Resolution&lt;/li&gt;
    &lt;li&gt;The ROS topic corresponding to this camera&lt;/li&gt;
    &lt;li&gt;T_cam_imu: The 4x4 transformation that takes a point from this camera&amp;rsquo;s IMU frame (where applicable) to this camera&amp;rsquo;s camera frame.&lt;/li&gt;
    &lt;li&gt;T_cn_cnm1: The 4x4 transformation that takes a point from this camera&amp;rsquo;s camera frame to the previous camera&amp;rsquo;s camera frame (e.g. cam2-&amp;gt;cam1, cam1-&amp;gt;cam0).&lt;/li&gt;
  &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Format</title>
      <link>https://daniilidis-group.github.io/mvsec/data_format/</link>
      <pubDate>Tue, 08 Aug 2017 21:07:13 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/mvsec/data_format/</guid>
      <description>

&lt;h2 id=&#34;ros-bag-data-format&#34;&gt;ROS Bag Data Format&lt;/h2&gt;

&lt;p&gt;Each sequence consists of a data ROS bag, with the following topics:
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;/davis/left/events&lt;/b&gt; (dvs_msgs/EventArray) - Events from the left DAVIS camera.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/left/image_raw&lt;/b&gt; (sensor_msgs/Image) - Grayscale images from the left DAVIS camera.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/left/imu&lt;/b&gt; (sensor_msgs/Imu) - IMU readings from the left DAVIS camera.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/right/events&lt;/b&gt; (dvs_msgs/EventArray) - Events from the right DAVIS camera.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/right/image_raw&lt;/b&gt; (sensor_msgs/Image) - Grayscale images from the right DAVIS camera.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/right/imu&lt;/b&gt; (sensor_msgs/Imu) - IMU readings from the right DAVIS camera.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/velodyne_point_cloud&lt;/b&gt; (sensor_msgs/PointCloud2) - Point clouds from the Velodyne (one per spin).&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/cam0/image_raw&lt;/b&gt; (sensor_msgs/Image) - Grayscale images from the left VI-Sensor camera.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/cam1/image_raw&lt;/b&gt; (sensor_msgs/Image) - Grayscale images from the right VI-Sensor camera.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/imu0&lt;/b&gt; (sensor_msgs/Imu) - IMU readings from the VI-Sensor.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/cust_imu0&lt;/b&gt; (visensor_node/visensor_imu) - Full IMU readings from the VI-Sensor (including magnetometer, temperature and pressure).&lt;/li&gt;
&lt;ul&gt;&lt;/p&gt;

&lt;p&gt;Two sets of custom messages are used, dvs_msgs/EventArray from &lt;a href=&#34;https://github.com/uzh-rpg/rpg_dvs_ros&#34;&gt;rpg_dvs_ros&lt;/a&gt; and visensor_node/visensor_imu from &lt;a href=&#34;https://github.com/ethz-asl/visensor_node&#34;&gt;visensor_node&lt;/a&gt;. The visensor_node package is optional if you do not need the extra IMU outputs (magnetometer, temperature and pressure.&lt;/p&gt;

&lt;p&gt;In addition, each corresponding ground truth bag contains the following topics:
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;/davis/left/depth_image_raw&lt;/b&gt; (sensor_msgs/Image) - Depth maps for the left DAVIS camera at a given timestamp (note, these images are saved using the CV_32FC1 format (i.e. floats).&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/left/blended_image_rect&lt;/b&gt; (sensor_msgs/Image) - Visualization of all events from the left DAVIS that are 25ms from each left depth map superimposed on the depth map. This message gives a preview of what each sequence looks like.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/left/odometry&lt;/b&gt; (geometry_msgs/PoseStamped) - Pose output using &lt;a href=&#34;https://www.ri.cmu.edu/publications/loam-lidar-odometry-and-mapping-in-real-time/&#34;&gt;LOAM&lt;/a&gt;. These poses are locally consistent, but may experience drift over time. Used to stitch point clouds together to generate depth maps.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/left/pose&lt;/b&gt; (geometry_msgs/PoseStamped) - Pose output using &lt;a href=&#34;https://google-cartographer-ros.readthedocs.io/en/latest/&#34;&gt;Google Cartographer&lt;/a&gt;. These poses are globally loop closed, and can be assumed to have minimal drift. Note that these poses were optimized using Cartographer&amp;rsquo;s 2D mapping, which does &lt;b&gt;not&lt;/b&gt; optimize over the height (Z axis). Pitch and roll are still optimized, however.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/right/depth_image_raw&lt;/b&gt; (sensor_msgs/Image) - Depth maps for the right DAVIS camera at a given timestamp.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;/davis/right/blended_image_rect&lt;/b&gt; (sensor_msgs/Image) - Visualization of all events from the right DAVIS that are 25ms from each right depth map superimposed on the depth map. This message gives a preview of what each sequence looks like.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;h2 id=&#34;text-format&#34;&gt;Text Format&lt;/h2&gt;

&lt;p&gt;Coming soon!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sample Code</title>
      <link>https://daniilidis-group.github.io/mvsec/sample_code/</link>
      <pubDate>Tue, 08 Aug 2017 21:07:13 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/mvsec/sample_code/</guid>
      <description>

&lt;h2 id=&#34;ros-bag-sample-code&#34;&gt;ROS Bag Sample Code&lt;/h2&gt;

&lt;p&gt;This sample code generates a ROS Subscriber that subscribes to the event stream, rectifies the events and integrates the rectified events over a fixed time window, publishing the integrated window. A rviz config will also be provided to easily visualize the data.&lt;/p&gt;

&lt;p&gt;Coming soon!&lt;/p&gt;

&lt;h2 id=&#34;text-file-sample-code&#34;&gt;Text File Sample Code&lt;/h2&gt;

&lt;p&gt;This sample code reads the event text file, rectifies the events and integrates the rectified events over a fixed time window, publishing the integrated window.&lt;/p&gt;

&lt;p&gt;Coming soon!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Download</title>
      <link>https://daniilidis-group.github.io/mvsec/download/</link>
      <pubDate>Tue, 08 Aug 2017 21:07:13 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/mvsec/download/</guid>
      <description>

&lt;h2 id=&#34;ros-bags&#34;&gt;ROS Bags&lt;/h2&gt;

&lt;p&gt;To process the bag files, you will need the &lt;a href=&#34;https://github.com/uzh-rpg/rpg_dvs_ros&#34;&gt;rpg_dvs_ros&lt;/a&gt; package to read the events (in particular dvs_msgs). You may also optionally install the &lt;a href=&#34;https://github.com/ethz-asl/visensor_node&#34;&gt;visensor_node&lt;/a&gt; to have access to the /cust_imu0 topic, which includes the magnetometer, pressure and temperature outputs of the VI-Sensor.&lt;/p&gt;

&lt;p&gt;Sequences will be added to this page on a rolling basis. We also plan to include videos for each sequence.&lt;/p&gt;

&lt;p&gt;Note that these bags are large (up to 17G).&lt;/p&gt;

&lt;div style=&#39;float:left;margin-left:5%&#39;&gt;
&lt;table&gt;
&lt;col width=&#34;30%&#34;&gt;
&lt;col width=&#34;30%&#34;&gt;
&lt;col width=&#34;30%&#34;&gt;
&lt;tr&gt;&lt;td&gt;Scene&lt;/td&gt;&lt;td&gt;Calibration&lt;/td&gt;&lt;td&gt;Sequence&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Indoor flying (Note: No VI-Sensor data is available for this scene).&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/camchain-imucam-indoor_flying.yaml&#34;&gt;Calibration&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
Indoor Flying 1 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flight1_data.bag&#34;&gt;Data (894M)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flight1_gt.bag&#34;&gt;Ground truth (1.7G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
Indoor Flying 2 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flight2_data.bag&#34;&gt;Data (1.1G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flight2_gt.bag&#34;&gt;Ground truth (2.0G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
Indoor Flying 3 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flight3_data.bag&#34;&gt;Data (1.2G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flight3_gt.bag&#34;&gt;Ground truth (2.3G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
Indoor Flying 4 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flight4_data.bag&#34;&gt;Data (241M)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/indoor_flying/indoor_flight4_gt.bag&#34;&gt;Ground truth (470M)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Outdoor Driving Day (Note: A hardware failure caused the grayscale images on the right DAVIS grayscale images for this scene to be corrupted. However, VI-Sensor grayscale images are available).&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/camchain-imucam-outdoor_day.yaml&#34;&gt;Calibration&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
West Philadelphia Day 1 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/west_philly_day1_data.bag&#34;&gt;Data (7.2G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/west_philly_day1_gt.bag&#34;&gt;Ground truth (6.1G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
West Philadelphia Day 2 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/west_philly_day2_data.bag&#34;&gt;Data (17G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_day/west_philly_day2_gt.bag&#34;&gt;Ground truth (15G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Outdoor Driving Night&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/camchain-imucam-outdoor_night.yaml&#34;&gt;Calibration&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
West Philadelphia Night 1 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/west_philly_night1_data.bag&#34;&gt;Data (6.0G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/west_philly_night1_gt.bag&#34;&gt;Ground truth (6.1G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
West Philadelphia Night 2 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/west_philly_night2_data.bag&#34;&gt;Data (8.7G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/west_philly_night2_gt.bag&#34;&gt;Ground truth (6.5G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
West Philadelphia Night 3 &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/west_philly_night3_data.bag&#34;&gt;Data (6.5G)&lt;/a&gt; &lt;a href=&#34;http://visiondata.cis.upenn.edu/mvsec/outdoor_night/west_philly_night3_gt.bag&#34;&gt;Ground truth (6.4G)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;BR CLEAR=&#34;all&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;text-files&#34;&gt;Text Files&lt;/h2&gt;

&lt;p&gt;Coming soon!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>